{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "CMD=[\"perl\", \"../english_testbed/data/2015/evaluate/correlation-noconfidence.pl\"]\n",
    "class Opts:\n",
    "    verbose=False\n",
    "    filter_test=\".*\"\n",
    "    \n",
    "opts=Opts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_2015_DATADIR=\"english_testbed/analysis/2015/training/results/\"\n",
    "TEST_2015_DATADIR=\"english_testbed/analysis/2015/testing/results\"\n",
    "TEST_2016_DATADIR=\"english_testbed/analysis/2016/testing/results\"\n",
    "TEST_2015_DIR=\"english_testbed/data/2015/test/\"\n",
    "TEST_2016_DIR=\"english_testbed/data/2016/test/\"\n",
    "TRAIN_2015_NAMEFILES=[\n",
    "  \"STS.2012.test.input.MSRpar.txt\",\n",
    "  \"STS.2012.test.input.MSRvid.txt\",\n",
    "  \"STS.2012.test.input.SMTeuroparl.txt\",\n",
    "  \"STS.2012.test.input.surprise.OnWN.txt\",\n",
    "  \"STS.2012.test.input.surprise.SMTnews.txt\",\n",
    "  \"STS.2012.train.input.MSRpar.txt\",\n",
    "  \"STS.2012.train.input.MSRvid.txt\",\n",
    "  \"STS.2012.train.input.SMTeuroparl.txt\",\n",
    "  \"STS.2013.test.input.FNWN.txt\",\n",
    "  \"STS.2013.test.input.headlines.txt\",\n",
    "  \"STS.2013.test.input.OnWN.txt\",\n",
    "  \"STS.2013.test.input.SMT.txt\",\n",
    "  \"STS.2014.test.input.deft-forum.txt\",\n",
    "  \"STS.2014.test.input.deft-news.txt\",\n",
    "  \"STS.2014.test.input.headlines.txt\",\n",
    "  \"STS.2014.test.input.images.txt\",\n",
    "  \"STS.2014.test.input.OnWN.txt\",\n",
    "  \"STS.2014.test.input.tweet-news.txt\"]\n",
    "TEST_2015_NAMEFILES=[\n",
    "  \"STS.input.answers-forums.txt\",\n",
    "  \"STS.input.answers-students.txt\",\n",
    "  \"STS.input.belief.txt\",\n",
    "  \"STS.input.headlines.txt\",\n",
    "  \"STS.input.images.txt\"]\n",
    "TEST_2016_NAMEFILES=[\n",
    "  \"STS2016.input.answer-answer.txt\",\n",
    "  \"STS2016.input.headlines.txt\",\n",
    "  \"STS2016.input.plagiarism.txt\",\n",
    "  \"STS2016.input.postediting.txt\",\n",
    "  \"STS2016.input.question-question.txt\"\n",
    "]\n",
    "\n",
    "RAW_2016_TRAINDIR=\"raw_train\"\n",
    "\n",
    "OUTPUT_DIR='output'\n",
    "\n",
    "CROSS_DIR=\"crosslingual_testbed/analysis/2016/testing/results\"\n",
    "CROSS_2016_NAMEFILES=[\n",
    "    \"STS.2016.input.crosslingual-news-part1.txt\",\n",
    "    \"STS.2016.input.crosslingual-news-part2.txt\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data_file(filename,nan=False):\n",
    "    GS=[]\n",
    "    DATA=[]\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            bits=line.split()\n",
    "            #print [x.split(':') for x in bits[1:]]\n",
    "            line=[ float(x.split(':')[1]) for x in bits[1:]]\n",
    "            line=np.nan_to_num(line)\n",
    "            line[line>200]=2\n",
    "            if not np.isfinite(line).all():\n",
    "                print \"problem\"\n",
    "                print line\n",
    "                print np.nan_to_num(line)\n",
    "                if nan:\n",
    "                   continue\n",
    "            GS.append(float(bits[0]))    \n",
    "            DATA.append(line)\n",
    "    return GS,np.array(DATA)\n",
    "\n",
    "def read_raw_file(filename,nan=False):\n",
    "    DATA=[]\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            bits=line.split(', ')\n",
    "            line=[ float(x) for x in bits]\n",
    "            line=np.nan_to_num(line)\n",
    "            line[line>200]=2\n",
    "            if not np.isfinite(line).all():\n",
    "                print \"problem\"\n",
    "                print line\n",
    "                print np.nan_to_num(line)\n",
    "                if nan:\n",
    "                   continue\n",
    "            DATA.append(line)\n",
    "    return np.array(DATA)\n",
    "\n",
    "\n",
    "def make_data_name(dirname,filename,replace=True):\n",
    "    if replace:\n",
    "        filename=filename.replace(\".txt\",\".dat\")\n",
    "    else:\n",
    "        filename=filename+\".dat\"\n",
    "    return os.path.join(\"..\",dirname,filename)\n",
    "\n",
    "def make_raw_name(dirname,filename):\n",
    "    filename=filename.replace(\".txt\",\".raw\")\n",
    "    return os.path.join(dirname,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape para STS.2012.test.input.MSRpar.txt (750, 16) (750, 7)\n",
      "Shape para STS.2012.test.input.MSRvid.txt (750, 16) (750, 7)\n",
      "Shape para STS.2012.test.input.SMTeuroparl.txt (459, 16) (459, 7)\n",
      "Shape para STS.2012.test.input.surprise.OnWN.txt (750, 16) (750, 7)\n",
      "Shape para STS.2012.test.input.surprise.SMTnews.txt (399, 16) (399, 7)\n",
      "Shape para STS.2012.train.input.MSRpar.txt (750, 16) (750, 7)\n",
      "Shape para STS.2012.train.input.MSRvid.txt (750, 16) (750, 7)\n",
      "Shape para STS.2012.train.input.SMTeuroparl.txt (734, 16) (734, 7)\n",
      "Shape para STS.2013.test.input.FNWN.txt (189, 16) (189, 7)\n",
      "Shape para STS.2013.test.input.headlines.txt (750, 16) (750, 7)\n",
      "Shape para STS.2013.test.input.OnWN.txt (561, 16) (561, 7)\n",
      "Shape para STS.2013.test.input.SMT.txt (750, 16) (750, 7)\n",
      "Shape para STS.2014.test.input.deft-forum.txt (450, 16) (450, 7)\n",
      "Shape para STS.2014.test.input.deft-news.txt (300, 16) (300, 7)\n",
      "Shape para STS.2014.test.input.headlines.txt (750, 16) (750, 7)\n",
      "Shape para STS.2014.test.input.images.txt (750, 16) (750, 7)\n",
      "Shape para STS.2014.test.input.OnWN.txt (750, 16) (750, 7)\n",
      "Shape para STS.2014.test.input.tweet-news.txt (750, 16) (750, 7)\n",
      "Shape para STS.input.answers-forums.txt (2000, 16) (2000, 7)\n",
      "Shape para STS.input.answers-students.txt (1500, 16) (1500, 7)\n",
      "Shape para STS.input.belief.txt (2000, 16) (2000, 7)\n",
      "Shape para STS.input.headlines.txt (1500, 16) (1500, 7)\n",
      "Shape para STS.input.images.txt (1500, 16) (1500, 7)\n",
      "Total train data files 2015: 18\n",
      "Total test  data files 2015: 5\n",
      "Total train raw files 2015: 18\n",
      "Total test  raw files 2015: 5\n"
     ]
    }
   ],
   "source": [
    "DATA_TRAIN_2015={}\n",
    "DATA_TEST_2015={}\n",
    "RAW_TRAIN_2015={}\n",
    "RAW_TEST_2015={}\n",
    "\n",
    "for filename in TRAIN_2015_NAMEFILES:\n",
    "    filename_=make_data_name(TRAIN_2015_DATADIR,filename)\n",
    "    gs,data=read_data_file(filename_,nan=True)\n",
    "    DATA_TRAIN_2015[filename]=(gs,data)\n",
    "    rawname=make_raw_name(RAW_2016_TRAINDIR,filename)\n",
    "    cols=read_raw_file(rawname)\n",
    "    RAW_TRAIN_2015[filename]=cols\n",
    "    print \"Shape para\",filename,data.shape, cols.shape\n",
    "\n",
    "for filename in TEST_2015_NAMEFILES:\n",
    "    filename_=make_data_name(TEST_2015_DATADIR,filename)\n",
    "    gs,data=read_data_file(filename_)\n",
    "    DATA_TEST_2015[filename]=(gs,data)\n",
    "    rawname=make_raw_name(RAW_2016_TRAINDIR,filename.replace('STS','STS.2015'))\n",
    "    cols=read_raw_file(rawname)\n",
    "    RAW_TEST_2015[filename]=cols\n",
    "    print \"Shape para\",filename,data.shape, cols.shape\n",
    "\n",
    "\n",
    "print \"Total train data files 2015:\",len(DATA_TRAIN_2015)\n",
    "print \"Total test  data files 2015:\",len(DATA_TEST_2015)   \n",
    "print \"Total train raw files 2015:\",len(RAW_TRAIN_2015)\n",
    "print \"Total test  raw files 2015:\",len(RAW_TEST_2015)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape (11342, 16)\n",
      "Training shape (11342,)\n"
     ]
    }
   ],
   "source": [
    "X_train_2015=[]\n",
    "Y_train_2015=[]\n",
    "X_test_2015=[]\n",
    "Y_test_2015=[]\n",
    "\n",
    "for filename,(gs,data) in DATA_TRAIN_2015.iteritems():\n",
    "    X_train_2015.append(data)\n",
    "    Y_train_2015.append(gs)\n",
    "X_train_2015=np.concatenate(X_train_2015,axis=0)\n",
    "Y_train_2015=np.concatenate(Y_train_2015,axis=0)\n",
    "\n",
    "\n",
    "print \"Training shape\", X_train_2015.shape\n",
    "print \"Training shape\", Y_train_2015.shape\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== For STS.input.answers-forums.txt\n",
      "PEA: 0.63449\n",
      "===== For STS.input.images.txt\n",
      "PEA: 0.81951\n",
      "===== For STS.input.answers-students.txt\n",
      "PEA: 0.64428\n",
      "===== For STS.input.headlines.txt\n",
      "PEA: 0.81535\n",
      "===== For STS.input.belief.txt\n",
      "PEA: 0.72874\n"
     ]
    }
   ],
   "source": [
    "# 2015 SOPA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "X_train_2015=np.nan_to_num(X_train_2015)\n",
    "rf= RandomForestRegressor(n_estimators=1000,warm_start=True)\n",
    "rf.fit(X_train_2015,Y_train_2015)\n",
    "\n",
    "\n",
    "for filename,(gs,data) in DATA_TEST_2015.iteritems():\n",
    "    gs_=rf.predict(data)\n",
    "\n",
    "    print \"===== For\",filename\n",
    "    #print \"EVS:\", explained_variance_score(gs,gs_)\n",
    "    #print \"MAE:\", mean_absolute_error(gs,gs_)\n",
    "    #print \"MSR:\", mean_squared_error(gs,gs_)\n",
    "    #print \"mAE:\", median_absolute_error(gs,gs_)\n",
    "    #print \"R2 :\", r2_score(gs,gs_)\n",
    "    filename_=os.path.join(\"..\",OUTPUT_DIR,filename)\n",
    "    testfilename=os.path.join(filename)\n",
    "    testfilename=testfilename.replace(\"STS.input\",\"STS.2015.gs\")\n",
    "\n",
    "    fn=open(filename_,'w')\n",
    "    for pred in gs_: \n",
    "        print >> fn, \"{0:1.1f}\".format(pred)\n",
    "    fn.close()\n",
    "    print \"PEA:\",utils.eval(CMD,os.path.join(\"..\",TEST_2015_DIR,testfilename),filename_)\n",
    "\n",
    "\n",
    "    \n",
    "    #===== For STS.input.answers-forums.txt\n",
    "    #PEA: 0.63331\n",
    "    #===== For STS.input.images.txt\n",
    "    #PEA: 0.81768\n",
    "    #===== For STS.input.answers-students.txt\n",
    "    #PEA: 0.63893\n",
    "    #===== For STS.input.headlines.txt\n",
    "    #PEA: 0.81666\n",
    "    #===== For STS.input.belief.txt\n",
    "    #PEA: 0.72389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STS.input.answers-forums.txt', 'STS.input.images.txt', 'STS.input.answers-students.txt', 'STS.input.headlines.txt', 'STS.input.belief.txt']\n",
      "Shape para STS2016.input.answer-answer.txt (1572, 16)\n",
      "Shape para STS2016.input.headlines.txt (1498, 16)\n",
      "Shape para STS2016.input.plagiarism.txt (1271, 16)\n",
      "Shape para STS2016.input.postediting.txt (3287, 16)\n",
      "Shape para STS2016.input.question-question.txt (1555, 16)\n",
      "Total train data files 2016: 23\n"
     ]
    }
   ],
   "source": [
    "DATA_TRAIN_2016={}\n",
    "DATA_TEST_2016={}\n",
    "RAW_TRAIN_2016={}\n",
    "RAW_TEST_2016={}\n",
    "\n",
    "print RAW_TEST_2015.keys()\n",
    "\n",
    "for filename,data in DATA_TRAIN_2015.iteritems():\n",
    "    DATA_TRAIN_2016[filename]=data\n",
    "    RAW_TRAIN_2016[filename]=RAW_TRAIN_2015[filename]\n",
    "for filename,data in DATA_TEST_2015.iteritems():\n",
    "    DATA_TRAIN_2016[filename]=data\n",
    "    RAW_TRAIN_2016[filename]=RAW_TEST_2015[filename]\n",
    "    \n",
    "for filename in TEST_2016_NAMEFILES:\n",
    "    filename_=make_data_name(TEST_2016_DATADIR,filename,replace=False)\n",
    "    gs,data=read_data_file(filename_,)\n",
    "    DATA_TEST_2016[filename]=(gs,data)\n",
    "    rawname=make_raw_name(RAW_2016_TRAINDIR,filename.replace('STS.','STS.2015.'))\n",
    "    cols=read_raw_file(rawname)\n",
    "    RAW_TEST_2016[filename]=cols\n",
    "    print \"Shape para\",filename,data.shape\n",
    "    \n",
    "print \"Total train data files 2016:\",len(DATA_TRAIN_2016)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape (19842, 16)\n",
      "Training shape (19842,)\n",
      "Testing shape STS2016.input.postediting.txt (3287, 16)\n",
      "Testing shape STS2016.input.question-question.txt (1555, 16)\n",
      "Testing shape STS2016.input.headlines.txt (1498, 16)\n",
      "Testing shape STS2016.input.plagiarism.txt (1271, 16)\n",
      "Testing shape STS2016.input.answer-answer.txt (1572, 16)\n"
     ]
    }
   ],
   "source": [
    "# 2016 SOPA\n",
    "X_train_2016=[]\n",
    "Y_train_2016=[]\n",
    "\n",
    "for filename,(gs,data) in DATA_TRAIN_2016.iteritems():\n",
    "    X_train_2016.append(data)\n",
    "    Y_train_2016.append(gs)\n",
    "X_train_2016=np.concatenate(X_train_2016,axis=0)\n",
    "Y_train_2016=np.concatenate(Y_train_2016,axis=0)\n",
    "\n",
    "\n",
    "print \"Training shape\", X_train_2016.shape\n",
    "print \"Training shape\", Y_train_2016.shape\n",
    "    \n",
    "\n",
    "X_train_2016=np.nan_to_num(X_train_2016)\n",
    "rf= RandomForestRegressor(n_estimators=1000)\n",
    "rf.fit(X_train_2016,Y_train_2016)\n",
    "\n",
    "\n",
    "for filename,(gs,data) in DATA_TEST_2016.iteritems():\n",
    "    gs_=rf.predict(data)\n",
    "    print \"Testing shape\",filename,data.shape\n",
    "    filename_=os.path.join(\"..\",OUTPUT_DIR,filename)\n",
    "    fn=open(filename_,'w')\n",
    "    for pred in gs_: \n",
    "        print >> fn, \"{0:1.3f}\".format(pred)\n",
    "    fn.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape (11342, 23)\n",
      "Training shape (11342,)\n"
     ]
    }
   ],
   "source": [
    "# 2015 + cols\n",
    "X_train_2015=[]\n",
    "Y_train_2015=[]\n",
    "X_test_2015=[]\n",
    "Y_test_2015=[]\n",
    "\n",
    "for filename,(gs,data) in DATA_TRAIN_2015.iteritems():\n",
    "    cols=RAW_TRAIN_2015[filename]\n",
    "    X_train_2015.append(np.hstack((data,cols)))\n",
    "    Y_train_2015.append(gs)\n",
    "X_train_2015=np.concatenate(X_train_2015,axis=0)\n",
    "Y_train_2015=np.concatenate(Y_train_2015,axis=0)\n",
    "\n",
    "\n",
    "print \"Training shape\", X_train_2015.shape\n",
    "print \"Training shape\", Y_train_2015.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== For STS.input.answers-forums.txt\n",
      "PEA: 0.65149\n",
      "===== For STS.input.images.txt\n",
      "PEA: 0.84402\n",
      "===== For STS.input.answers-students.txt\n",
      "PEA: 0.70415\n",
      "===== For STS.input.headlines.txt\n",
      "PEA: 0.82523\n",
      "===== For STS.input.belief.txt\n",
      "PEA: 0.7338\n"
     ]
    }
   ],
   "source": [
    "X_train_2015=np.nan_to_num(X_train_2015)\n",
    "rf= RandomForestRegressor(n_estimators=1000,warm_start=True)\n",
    "rf.fit(X_train_2015,Y_train_2015)\n",
    "\n",
    "\n",
    "for filename,(gs,data) in DATA_TEST_2015.iteritems():\n",
    "    cols=RAW_TEST_2015[filename]\n",
    "    gs_=rf.predict(np.hstack((data,cols)))\n",
    "\n",
    "    print \"===== For\",filename\n",
    "    #print \"EVS:\", explained_variance_score(gs,gs_)\n",
    "    #print \"MAE:\", mean_absolute_error(gs,gs_)\n",
    "    #print \"MSR:\", mean_squared_error(gs,gs_)\n",
    "    #print \"mAE:\", median_absolute_error(gs,gs_)\n",
    "    #print \"R2 :\", r2_score(gs,gs_)\n",
    "    filename_=os.path.join(\"..\",OUTPUT_DIR,filename)\n",
    "    testfilename=os.path.join(filename)\n",
    "    testfilename=testfilename.replace(\"STS.input\",\"STS.2015.gs\")\n",
    "\n",
    "    fn=open(filename_,'w')\n",
    "    for pred in gs_: \n",
    "        print >> fn, \"{0:1.1f}\".format(pred)\n",
    "    fn.close()\n",
    "    print \"PEA:\",utils.eval(CMD,os.path.join(\"..\",TEST_2015_DIR,testfilename),filename_)\n",
    "\n",
    "\n",
    "#===== For STS.input.answers-forums.txt\n",
    "#PEA: 0.65703\n",
    "#===== For STS.input.images.txt\n",
    "#PEA: 0.84109\n",
    "#===== For STS.input.answers-students.txt\n",
    "#PEA: 0.70356\n",
    "#===== For STS.input.headlines.txt\n",
    "#PEA: 0.8247\n",
    "#===== For STS.input.belief.txt\n",
    "#PEA: 0.73149\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape (19842, 23)\n",
      "Training shape (19842,)\n",
      "Testing shape STS2016.input.postediting.txt (3287, 23)\n",
      "Testing shape STS2016.input.question-question.txt (1555, 23)\n",
      "Testing shape STS2016.input.headlines.txt (1498, 23)\n",
      "Testing shape STS2016.input.plagiarism.txt (1271, 23)\n",
      "Testing shape STS2016.input.answer-answer.txt (1572, 23)\n"
     ]
    }
   ],
   "source": [
    "# 2016 SOPA + cols\n",
    "X_train_2016=[]\n",
    "Y_train_2016=[]\n",
    "\n",
    "for filename,(gs,data) in DATA_TRAIN_2016.iteritems():\n",
    "    cols=RAW_TRAIN_2016[filename]\n",
    "    X_train_2016.append(np.hstack((data,cols)))\n",
    "    Y_train_2016.append(gs)\n",
    "X_train_2016=np.concatenate(X_train_2016,axis=0)\n",
    "Y_train_2016=np.concatenate(Y_train_2016,axis=0)\n",
    "\n",
    "\n",
    "print \"Training shape\", X_train_2016.shape\n",
    "print \"Training shape\", Y_train_2016.shape\n",
    "    \n",
    "\n",
    "X_train_2016=np.nan_to_num(X_train_2016)\n",
    "rf= RandomForestRegressor(n_estimators=100)\n",
    "rf.fit(X_train_2016,Y_train_2016)\n",
    "\n",
    "\n",
    "for filename,(gs,data) in DATA_TEST_2016.iteritems():\n",
    "\n",
    "    cols=RAW_TEST_2016[filename]\n",
    "    data=np.hstack((data,cols))\n",
    "    data=np.nan_to_num(data)\n",
    "    gs_=rf.predict(data)\n",
    "    print \"Testing shape\",filename,data.shape\n",
    "    filename_=os.path.join(\"..\",OUTPUT_DIR,filename)\n",
    "    fn=open(filename_,'w')\n",
    "    for pred in gs_: \n",
    "        print >> fn, \"{0:1.3f}\".format(pred)\n",
    "    fn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape para STS.2016.input.crosslingual-news-part1.txt (301, 16)\n",
      "Shape para STS.2016.input.crosslingual-news-part2.txt (2973, 16)\n"
     ]
    }
   ],
   "source": [
    "CROSS_TEST_2016={}\n",
    "\n",
    "\n",
    "for filename in CROSS_2016_NAMEFILES:\n",
    "    filename_=make_data_name(CROSS_DIR,filename,replace=False)\n",
    "    gs,data=read_data_file(filename_)\n",
    "    CROSS_TEST_2016[filename]=(gs,data)\n",
    "    print \"Shape para\",filename,data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape (19842, 16)\n",
      "Training shape (19842,)\n",
      "Testing shape STS.2016.input.crosslingual-news-part2.txt (2973, 16)\n",
      "Testing shape STS.2016.input.crosslingual-news-part1.txt (301, 16)\n"
     ]
    }
   ],
   "source": [
    "# 2016 SOPA -> CROSS\n",
    "X_train_2016=[]\n",
    "Y_train_2016=[]\n",
    "\n",
    "for filename,(gs,data) in DATA_TRAIN_2016.iteritems():\n",
    "    X_train_2016.append(data)\n",
    "    Y_train_2016.append(gs)\n",
    "X_train_2016=np.concatenate(X_train_2016,axis=0)\n",
    "Y_train_2016=np.concatenate(Y_train_2016,axis=0)\n",
    "\n",
    "\n",
    "print \"Training shape\", X_train_2016.shape\n",
    "print \"Training shape\", Y_train_2016.shape\n",
    "    \n",
    "\n",
    "X_train_2016=np.nan_to_num(X_train_2016)\n",
    "rf= RandomForestRegressor(n_estimators=1000)\n",
    "rf.fit(X_train_2016,Y_train_2016)\n",
    "\n",
    "\n",
    "for filename,(gs,data) in CROSS_TEST_2016.iteritems():\n",
    "    gs_=rf.predict(data)\n",
    "    print \"Testing shape\",filename,data.shape\n",
    "    filename_=os.path.join(\"..\",OUTPUT_DIR,filename)\n",
    "    fn=open(filename_,'w')\n",
    "    for pred in gs_: \n",
    "        print >> fn, \"{0:1.3f}\".format(pred)\n",
    "    fn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
