{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import libraries, prepare verbose and set options\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import codecs\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def verbose(*args):\n",
    "    print \" \".join([str(a) for a in args])\n",
    "\n",
    "class Opts:\n",
    "    verbose=False\n",
    "    filter_test=\".*\"\n",
    "    \n",
    "opts=Opts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_phrases_from_file(dirname,filename,format='2017',translation=False):\n",
    "    re_file=re.compile('.*\\.input\\..*\\.txt$')                                        \n",
    "    re_file_translation=re.compile('.*\\.input\\..*\\.translation.txt$')  \n",
    "    if translation:                                                              \n",
    "        re_file=re_file_translation\n",
    "\n",
    "    phrases=[]                                                                   \n",
    "    if not re_file.match(filename):                                              \n",
    "        return []                                                                \n",
    "                                                                                 \n",
    "    with codecs.open(os.path.join(dirname,filename),encoding='utf-8') as data:\n",
    "        for line in data:\n",
    "            bits=line.strip().split('\\t')\n",
    "            if len(bits)>=2 or len(bits)<=4:                                     \n",
    "                if not format:                                                   \n",
    "                    phrases.append((bits[0],bits[1]))                            \n",
    "                elif format==\"2017\":                                             \n",
    "                    phrases.append((bits[2],bits[3]))                            \n",
    "    return phrases \n",
    "\n",
    "def load_gs_from_file(dirname,filename):\n",
    "    re_gs=re.compile('.*\\.gs\\..*\\.(txt|ascii)$')\n",
    "    gs=[]\n",
    "    if not re_gs.match(filename):\n",
    "        return []\n",
    "\n",
    "    with open(os.path.join(dirname,filename)) as data:\n",
    "        for line in data:\n",
    "            line=line.strip()\n",
    "            try:\n",
    "                gs.append(float(line))\n",
    "            except ValueError:\n",
    "                gs.append(0.0)\n",
    "    return gs\n",
    "\n",
    "def load_all_phrases(dirname,filter=\".\",format=None,translation=False):          \n",
    "    all_phrases=[]                                                               \n",
    "    filter_dirs=re.compile(filter)                                               \n",
    "    for filename in os.listdir(dirname):                                         \n",
    "        if not filter_dirs.search(filename):                                     \n",
    "            continue                                                             \n",
    "        phrases=load_phrases_from_file(dirname,filename,format=format,translation=translation)\n",
    "        if len(phrases)>0:                                                       \n",
    "            all_phrases.append((filename,phrases))                               \n",
    "    return all_phrases\n",
    "\n",
    "def load_all_gs(dirname):\n",
    "    all_gs=[]\n",
    "    for filename in os.listdir(dirname):\n",
    "        gs=load_gs_from_file(dirname,filename)\n",
    "        if len(gs)>0:\n",
    "            all_gs.append((filename,gs))\n",
    "    return all_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "YEAR=\"2017\"\n",
    "MAX_NB_WORDS=5000\n",
    "MAX_SEQUENCE_LENGTH=30\n",
    "VALIDATION_SPLIT=0.30\n",
    "GLOVE_DIR='.'\n",
    "EMBEDDING_DIM=100\n",
    "TRAIN_DIRS=[\n",
    "    (\"../spanish_testbed/data/\"+YEAR,None,True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Phrases in STS.2012.test.input.MSRvid.translation.txt 750 750\n",
      "Phrases in STS.2014.test.input.news.translation.txt 480 480\n",
      "Phrases in STS.2013.test.input.FNWN.translation.txt 189 189\n",
      "Phrases in STS.2012.test.input.MSRpar.translation.txt 750 750\n",
      "Phrases in STS.2013.test.input.headlines.translation.txt 619 750\n",
      "Phrases in STS.2012.test.input.SMTeuroparl.translation.txt 459 459\n",
      "Phrases in STS.2012.test.input.surprise.SMTnews.translation.txt 399 399\n",
      "Phrases in STS.2012.train.input.SMTeuroparl.translation.txt 734 734\n",
      "Phrases in STS.2014.input.local2.sentences.translation.txt 100 101\n",
      "Phrases in STS.2012.train.input.MSRpar.translation.txt 750 750\n",
      "Phrases in STS.2014.test.input.wikipedia.translation.txt 324 324\n",
      "Phrases in STS.2015.train.input.wikipedia.translation.txt 251 251\n",
      "Phrases in STS.2014.train.input.li65.translation.txt 65 65\n",
      "Phrases in STS.2012.train.input.MSRvid.translation.txt 90 750\n",
      "Phrases in STS.2014.input.local.sentences.translation.txt 101 101\n",
      "Phrases in STS.2013.test.input.OnWN.translation.txt 212 561\n",
      "Phrases in STS.2014.input.local.pairs-journals.translation.txt 106 133\n",
      "Phrases in STS.2015.train.input.newswire.translation.txt 500 500\n",
      "Total train phrases ../spanish_testbed/data/2017 6879\n",
      "Total train phrases 13758\n"
     ]
    }
   ],
   "source": [
    "def load_train_dirs(dirs):\n",
    "    train_data=[]\n",
    "    gs_data=[]\n",
    "    for directory,format,translation in dirs: \n",
    "        verbose('Starting training')\n",
    "        train_data_=load_all_phrases(os.path.join(directory,'train'),format=format,translation=True)\n",
    "        gs_data_=dict(load_all_gs(os.path.join(directory,'train')))\n",
    "\n",
    "        for (n,d) in train_data_:\n",
    "            n_=n.replace('input', 'gs')\n",
    "            if translation:\n",
    "                n_=n_.replace('.translation', '')\n",
    "            for i,s in enumerate(d):\n",
    "                train_data.append(s[0].encode('utf-8'))\n",
    "                train_data.append(s[1].encode('utf-8'))\n",
    "                gs_data.append(gs_data_[n_][i])\n",
    "            verbose(\"Phrases in\",n,len(d),len(gs_data_[n_]))\n",
    "        verbose('Total train phrases',directory,sum([len(d) for n,d in train_data_]))\n",
    "        \n",
    "        \n",
    "        verbose('Total train phrases',len(train_data))\n",
    "    return train_data,gs_data\n",
    "    \n",
    "train_data,gs_data=load_train_dirs(TRAIN_DIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN not available)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16065 unique tokens.\n",
      "('Shape of data tensor:', (6879, 60))\n",
      "('Shape of label tensor:', (6879, 6))\n",
      "('Shape of train:', (4816, 60))\n",
      "('Shape of train:', (4816, 6))\n",
      "('Shape of train:', (2063, 60))\n",
      "('Shape of train:', (2063, 6))\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "sequences = tokenizer.texts_to_sequences(train_data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data_ = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data=np.zeros((data_.shape[0]/2,data_.shape[1]*2))\n",
    "\n",
    "for i in range(data_.shape[0]/2):\n",
    "    data[i,:data_.shape[1]]=data_[2*i]\n",
    "    data[i,data_.shape[1]:]=data_[2*i+1]\n",
    "\n",
    "labels = to_categorical(np.asarray(gs_data))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Shape of train:',x_train.shape)\n",
    "print('Shape of train:',y_train.shape)\n",
    "print('Shape of train:',x_val.shape)\n",
    "print('Shape of train:',y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-0.038194   -0.24487001  0.72812003 ..., -0.1459      0.82779998\n",
      "   0.27061999]\n",
      " [-0.1529     -0.24279     0.89837003 ..., -0.59100002  1.00390005\n",
      "   0.20664001]\n",
      " ..., \n",
      " [ 0.12258    -0.5521     -0.85820001 ...,  0.19174001 -0.69847    -0.33796   ]\n",
      " [-0.010992    0.54471999 -0.18803    ..., -0.49559999 -0.065065   -0.036046  ]\n",
      " [-0.11792     0.53746003  0.90812999 ..., -0.025289    0.45811999\n",
      "   0.74392998]]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_9 (Embedding)          (None, 60, 100)       1606600     embedding_input_6[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_7 (Convolution1D)  (None, 60, 32)        9632        embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_7 (MaxPooling1D)    (None, 30, 32)        0           convolution1d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                    (None, 100)           53200       maxpooling1d_7[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 6)             606         lstm_5[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 1,670,038\n",
      "Trainable params: 63,438\n",
      "Non-trainable params: 1,606,600\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH*2,\n",
    "                             dropout=0.2,\n",
    "                            trainable=False)\n",
    "\n",
    "#sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH*2,), dtype='int32')\n",
    "#embedded_sequences = embedding_layer(sequence_input)\n",
    "#x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "#x = MaxPooling1D(5)(x)\n",
    "#x = Conv1D(96, 5, activation='relu')(x)\n",
    "#x = MaxPooling1D(5)(x)\n",
    "#x = Conv1D(86, 5, activation='relu')(x)\n",
    "#x = MaxPooling1D(5)(x)  # global max pooling\n",
    "#x = Flatten()(x)\n",
    "#x = Dense(64, activation='relu')(x)\n",
    "#preds = Dense(6, activation='softmax')(x)\n",
    "#model = Model(sequence_input, preds)\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer='rmsprop',\n",
    "#              metrics=['acc'])\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(LSTM(100, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# happy learning!\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=30, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
